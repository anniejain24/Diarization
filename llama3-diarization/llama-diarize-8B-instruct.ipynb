{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcbc3143c6041dd92aed2c0f956611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' the Interwebs! Me and me trusty crew o' code have been sailin' the digital waters fer years, spreadin' the word o' piratey wisdom and havin' a swashbucklin' good time doin' it! So hoist the Jolly Roger and set course fer a treasure trove o' pirate chat, matey!\"}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"/archive/shared/sim_center/shared/annie/hf_models/8b-instruct\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_id = '10_0991_331330'\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from typing import List\n",
    "\n",
    "def read_transcript_from_id(transcript_id: str, chunk_num: int=1)->List[str]:\n",
    "\n",
    "    path_to_data_folder = '/archive/shared/sim_center/shared/ameer/'\n",
    "    # path_to_data_folder = '/archive/shared/sim_center/shared/annie/GPT4 3-chunk/'\n",
    "    # lookinto this dictionary to find the path\n",
    "    # can also manually create the path and it would be faster but not by much\n",
    "\n",
    "\n",
    "    merged_lookup = pd.read_csv(path_to_data_folder + 'grade_lookupv5.csv')\n",
    "\n",
    "    path = merged_lookup[merged_lookup.id == transcript_id].path.iloc[0]\n",
    "\n",
    "    path = path[:-4] + '.json'\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open(path)\n",
    "\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    json_transcript = json.load(f)\n",
    "\n",
    "    transcript = []\n",
    "    transcript_txt = ''\n",
    "\n",
    "    lines = json_transcript\n",
    "    \n",
    "    if chunk_num == 1: \n",
    "        for line in lines:\n",
    "            if line['text'] != '\\n':\n",
    "                tok_line = line['text'].split(' ')\n",
    "                for i in range(len(tok_line)):\n",
    "                    transcript_txt += ' ' + tok_line[i]\n",
    "        transcript.append(transcript_txt)\n",
    "    \n",
    "    else:\n",
    "        transcript_chunks = []\n",
    "        # for each chunk\n",
    "        for n in range(chunk_num):\n",
    "            transcript = ''\n",
    "            # get the relevant lines\n",
    "            start = n*int(len(lines)/chunk_num)\n",
    "            end = (n+1)*int(len(lines)/chunk_num)\n",
    "            if n == chunk_num-1: end = len(lines)\n",
    "\n",
    "            for line in lines[start: end]:\n",
    "                if line['text'] != '\\n':\n",
    "                    tok_line = line['text'].split(' ')\n",
    "                    for i in range(len(tok_line)):\n",
    "                        transcript += ' ' + tok_line[i]\n",
    "            #append to transcript\n",
    "            transcript_chunks.append(transcript)\n",
    "        \n",
    "        transcript = transcript_chunks\n",
    "\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"  Learners, you may begin your patient station, remove your cover sheet, jot down any... Thank you. Come in.  Come on in, man.  Hello, Mr. Pimbleton. Hi, my name is Coleman. I'm a medical student at UT Southwestern. I am here to ask you some questions. I just want to know how you are feeling today.  I've been feeling sick to my stomach.  Now I throw one up, I feel not good.  Okay, when did this start?  It started getting nauseous.  I was feeling nausea.  I had a stomachache.  I felt nause up. I am not good.  Okay, when did this start? I started getting nauseous two days ago. Threw up the first  time yesterday after breakfast, about 20 minutes after I ate. Any other episodes of vomiting?  Yeah, I threw up three or four times yesterday and then after breakfast today about five  minutes after I ate.  Okay.  And what does your vomit look like? It's just small bits of undigested food, whatever I eat.  Any blood in it?  No.  No?  No  Bile?  No  Just the chunks of food?  No  Okay  Now this has been going on for just two days.  Has this happened before?  No, never.  Okay.  And does anything make it worse?  Uh, eating.  Yeah.  Yeah, anything makes it better?  Not really, other than not eating  Okay Is there any associated with the vomiting?  Yeah, I got a little mild pain in my abdomen there.  Okay, on a scale of 1 to 10, about how bad, how would you describe it?  Oh, it's like a 3.  Yeah. Just kind of an irritation.  Okay.  And so if you don't eat, do you get nauseous still?  Oh, yeah.  So nausea is there all the time.  Okay, nauseas.  Any fevers?  No, no fever.  You mean chills? Chills?  No.  If you don't mind me asking, what are you usually eating at these meals?  I don' know, like for breakfast I usually have scrambled eggs and bacon and toast.  Okay.  No changes in your diet?  No change in diet.  Okay, okay.  Okay No changes in your diet? No change in diet. Okay.  Have you taken any medication for this? Over the counter medications?  No.  Alrighty.  Anything else  that you have to share that would help us like diagnose what this is any  any other changes any others that that's you're having at home no no nothing specific nothing  comes to mind i'm sorry another comes up on mine okay and then i imagine loss of appetite  I imagine loss of appetite as well. Yeah, it's decreased as you start throwing up.  Okay, if I could just ask you a few questions now about your past medical history.  Sure.  Have you been diagnosed with any chronic illnesses?  Oh, let's see.  Hypertension.  I've got osteoarthritis in both my knees.  I've got osteoarthritis in both my knees and then I get my lower back, I have chronic back pain from that. I think that's all.  Do you take any medications at home?  Yeah,  I take verapamil, they prescribe up for hypertension. I take hydrocortone and Tylenol.  That helps my back and my knees, actually.  Okay.  I'll ask you a few questions now  about your surgical history.  Have you had any prior surgeries?  No.  And I want to ask you some questions about  your family history? Has anyone in your  family been diagnosed with a chronic illness or disease?  No, my parents both died of old age.  My siblings are in good health.  Good to hear. That's good to  hear, that's phenomenal. And then I'd just like to ask you some questions lastly about social history.  Any alcohol use? No.  Any tobacco products used?  No  Any drugs?  Drugs never used.  Just what I told you.  Just these medications?  Yeah  Do you feel safe at home? I'm sorry? Yeah!  Do you get married?  Yeah  Or wife and I at  home.  Yeah  Good to hear. Good. At this point I would like to perform a brief physical exam.  If I could have you sit on the on a Sure. You don't have to lay back just yet.  All right.  I would like...  I'm sorry.  I still need these rooms.  How's your day been otherwise besides the... Man, if I get rid of this throwing up,  it'd be better.  Yeah.  I can't imagine that's comfortable.  No, it's not.  What do you do for work?  Oh, I'm retired.  Yeah, but you don't mind me asking your age. How old are you? What do you do for work? Oh, I'm retired. Oh.  Yeah.  If you don't mind me asking your age, how old are you?  55.  55...  That's nice, that's early.  You're retired?  What did you before that?  I worked for General Motors.  Oh yeah, Chimbley Live.? I worked for General Motors.  On the assembly line.  Oh, I'm a Ford guy.  Sorry.  That's okay.  Just gonna take a look.  Deep breath. Okay. No back pain? No. Just lower back from the... Yeah.  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay  Okay Okay Okay Okay Okay Yeah. Okay. And then if you don't mind at this point, I'll have you lay back on the table.  Alright.  We'll do an abdominal exam.  Alright  First, if don' mind, we're going to lift up your tap. Just look.  That's fine.  Okay.  Alright.  Let's take a listen. Do a little palpation, tell me if you feel anything, okay?  Alright.  Any pain?  Nope.  Pain?  Nope?  Any painting?  Nope!  Pain here?  Nope  Alright, Mr. Pribbleton, why don't you sit back up.  Alright.  And if you would like to feel more comfortable you can stay seated there or go back to the  chair, whichever one you prefer.  Well.  You don t have to worry, it might be something serious.  Alright well, that s never all the way off the table, but I can certainly say when it's further down on the  table. Let's just go over real quickly what I think you might have and what could be causing  these symptoms. Okay. The way you describe your vomiting, sounds like it's not getting very far down your digestive tract  so that makes me start to think of things that are closer to the stomach in a song place  and it is possible some people over time they can get that constriction of their esophagus  which makes it less uh dilated I guess for food to pass through  and so we can do something it's called a barium swallow study and we would just  look at to make sure that your esopagus goes all the way down to your stomach  and that there's no problems there another option although it  likely you could have an obstruction CT scan of your out just to see if there's  anything any masses or changes that could be blocking that from your side  and then something else if we still can't figure out what's going on we  could also do an endoscopy where GI doctors would put a camera down your  throat just see it there any any ulcers or concerning changes  but your lack of fevers and other symptoms make me think that this is either something to do  with the digestive tract or make quick viral gastric viral bug okay Oh, okay. So I'm not concerned for any serious infections.  Do you have any questions about that?  No, I guess not.  Okay.  I just want to get it taken care of so I don't feel better.  So most likely it could resolve on its own,  but we're going to do those tests to make sure that it's not anything else that could  Be alarming good. Does that sound like a plan that you're you bet comfortable with sure Paul through that? Yeah. Oh, yeah  Do you have any questions? I?  Guess not  Okay, get this done today. Thank you. Thank You. It was good to see you!  Okay, take care. I'm going to put it in the fridge with a spoon. I'll try it. I can't eat this.  I have to do it again.  I will try again with my hands.  I don't know if I should do this or not.  I think I did it right.  I ate it all.  I was hungry.  I want to go to the bathroom.  I need to take a shower.  I am hungry, too.  I feel like I've been eating too much. I eat I'm going to go ahead and do that. so so Thank you. Learners, your time is up.  Please exit your patient station.  Log into SimIQ using your UT credentials. Okay. Thank you.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_transcript_from_id(transcript_id, chunk_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(transcript):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please summarize the following text in one paragraph. 100 words. Do not add any information that is not in the text\"},\n",
    "    {\"role\": \"user\", \"content\": transcript},\n",
    "]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(read_transcript_from_id(transcript_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the text in one paragraph:\\n\\nA medical student, Coleman, is conducting a patient station with Mr. Pimbleton, a 55-year-old retired man who has been experiencing nausea, vomiting, and stomachache for two days. Mr. Pimbleton reports that his vomiting is accompanied by small bits of undigested food and is triggered by eating. He has no fever, chills, or blood in his vomit, and has not experienced this before. Coleman performs a brief physical exam and discusses possible causes of the symptoms, including a possible obstruction or constriction of the esophagus. He recommends further testing, including a barium swallow study, CT scan, or endoscopy, to determine the cause of the symptoms and rule out any serious infections.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   Learners, you may begin your patient station.  Please remove your cover sheet, jot down any notes, knock, and then enter. Thank you. so  so I don't know. Hello?  Hi, this is Mrs. Miller.  Yes, yes it is.  Hi Mrs Miller, my name is Jackson Agroz.  I'm the medical student working at the clinic today.  I am calling to speak about your daughter.  What was your name again?  Jackson Agrroz.  Okay.  Your S-47024.  That's your OSCE number?  427024, yes.  Okay, well, OSCENumber21, sorry.  There we go, thank you.  Well, I am glad you called.  I was very worried because my daughter has had a cough for a long time,  and it just seems it won't go away.  Okay.  How long has it been going on for?  Five weeks is when it started.  Okay, and can you describe the cough to me?  It's a dry cough that's almost nonstop.  Okay?  And you said it's been getting worse over these five weeks? Yes. At first it was on and off, but now it is constant.  Okay, has she been vomiting at all? No. Has she has been sick other than the  cough? Any fevers, chills, runny nose, anything like that?  Well, five weeks ago she had had a fever and some nasal congestion and the  cough, and it lasted two days.  But the coughing is the only thing that didn't go away.  Okay.  So was she seen in clinic at that time,  or did she seem like her symptoms got better except for the cough?  It seemed that her systems got worse.  Her symptoms went better.  Well, the coughing and the head congestion went away.  Do you think she'll need like an antibiotic or something?  So why don't we, I'll kind of talk you through what I'm thinking of towards the end if you don'T mind.  I'M just trying to flesh out everything and make sure I don' t miss anything.  Does that make sense?  Yes.  I appreciate your concern, though.  I know that it's got to be scary for you.  Yes  Has she been doing okay in school with the cough or has she just been staying home?  She's been going to school. She went to the school at the cough, but the last week she stayed out of gym class.  Okay.  Is the coughing worse with running or activity or anything like that?  Yes, very, much worse.  With the running and the playing with friends.  Okay, has she ever had issues with exercise in the past?  I don't understand.  Like, for example, when she runs, does she  ever have coughing fits or anything like that? Yes.  Okay, and that has happened in  the last five weeks?  Or just these past five  weeks. These past  five  weeks  Okay and since the initial start she  hasn't had any more fevers or chills or  anything like that? No, nothing like That. Has there been anybody sick around her?  No one around here has been sick at all. Before she got sick initially, do you remember if  she was, if anybody around had been Sick?  No. Not that I know of. Okay. All right.  I'm just going to move on to her past medical history.  Has Bailey been ill?  Has she been in the past?  Does she have any medical conditions, I should say?  No.  No nothing like that.  She had like a bout of eczema around two years old.  And every once in a while she'll get itchy eyes, nose, throat.  Okay, does she take anything for allergies?  No.  Okay, any surgeries that she's had in the past?  No, okay.  And as far as family history goes, has there ever been a history of asthma in  the family?  Her dad has a story of as a child, but not now.  Okay?  Okay any family histories of high blood pressure? Her dad has a history of asthma as a child, but not now. Okay.  Any family history of high blood pressure, diabetes?  No.  No?  Okay, and does she live at home with you and her father?  She does.  My uncle just, I mean her uncle, my brother just moved in with us about a month ago.  Okay does anybody in the us about a month ago. Okay.  Does anybody in the house smoke?  Only outside.  Okay, and who smokes?  Your brother or?  Yeah, me. My brother and her dad.  Okay and does she have any siblings?  No.  Okay any pets at home?  We have a dog and two cats,  and they go inside and out.  Okay?  And Bailey hasn't ever had any issues  with the dog or cats before?  No, no.  Okay Ms. Miller,  so just to kind of,  do you mind if I summarize what I've heard of before and just so you can interject if  I'm missing anything?  Okay.  So Bailey, she's seven years old, previously healthy. About five weeks ago,  she had this initial kind  of fever, runny nose. But ever since then,  she has this worsening cough  that's kind  of progressing and now  preventing her from doing  everything that she wants to do. No past medical history except for some eczema and maybe some  runny nose occasionally. And then her father had asthma. Anything else in particular I'm missing?  No, not that I can think of. So just to walk you through, Ms. Miller, what I'm thinking, I think it's possible that your daughter might have some asthma.  She's, especially the fact that there is some smoke exposure.  I know that, I mean,  I don't know if you've heard of it,  but I do know,  I feel that you're family members are smoking outside and that can help  lessen her exposure,  but still  considering that she's got a family history of family  history asthma and past medical history those tend to kind of run together with  asthma it's not definite that that's this is what she has but I would advise  coming in to get further pulmonary workup for her to see if she needs an inhaler or something.  Okay.  Does that sound agreeable to you?  Is that something that you can do to bring her in sometime?  Yeah.  Okay, perfect.  Yeah, absolutely.  We can try to make an appointment as soon as possible.  It's also possible that she could have just had a viral illness,  considering her fever and chills and cough.  Those also tend to run together.  And cough is something that it usually lasts much longer than the other two  and can get really irritating.  So it's possible this is just a virus.  It could be a real illness.  But I would rather be safer than sorry  and make sure she gets checked out for asthma as well.  Okay. That's great. I'm really worried about this.  I'd like to get it cleared up.  Yeah, absolutely. We can see you as soon as possible.  Okay, any other questions you have?  No, I think I've got it.  All right. Thank you so much.  Thank your call. Thanks. Bye. All right. I'm going to turn it over to you, and I'll see you in a minute. Thank you.  Thank  you for  your time.  Thank  you  for  your  time,  and  I hope okay Thank you. Learners, you have five minutes remaining. so I'm going to go ahead and do that. so  so so so  so so so so  okay I'm going to go ahead and do that. so Thank you. so  so I'm going to go ahead and do that. so Thank you. Learners, your time is up.  Please exit your room and log in to SimIQ using your UT credentials. Thank you.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_transcript_from_id(id_set1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize(summary, transcript):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": '''The following text represents an audio transcript between a medical student and a patient, with the summary included below.\n",
    "                The student is the one who will be playing the role of doctor, asking questions about the patient's condition and symptoms.\n",
    "                Please diarize the following transcript in order to indicate who speaks when, using the format in the following example, with the label \"Student\" for the medical student and \"Patient\" for the patient (not their names):\n",
    "\n",
    "                Student: Hello, how are you today?\n",
    "\n",
    "                Patient: I am feeling sick. \n",
    "\n",
    "                Student: Oh no, how long have you felt sick? \n",
    "\n",
    "                Patient: About two days. \n",
    "\n",
    "                Separate speaker turns by an extra new line, as above ('\\n\\n'). The lines do not always have to be alternating labels. There may be consecutive lines from one speaker. Do not use any speaker labels for the two participants other than \"Student\" and \"Patient.\"\n",
    "\n",
    "                The transcript may contain some instructions for the student, coming from a third-party speaker. Please label this segment \"Instructions: \". \n",
    "                If there are no instructions in the transcript, disregard this label. \n",
    "\n",
    "                IMPORTANT: Do not remove or add any words to the transcript other than the speaker labels. Also, do not add or remove any punctuation or change any spellings.'''},\n",
    "    {\"role\": \"user\", \"content\": 'summary: ' + summary[\"content\"] + '\\n\\ntranscript: ' + transcript},\n",
    "]      \n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=10000,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][-1]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'I\\'ve diarized the transcript with the labels \"Student\" for the medical student and \"Patient\" for the patient. I\\'ve also added the label \"Instructions:\" for the third-party speaker\\'s instructions. Here is the diarized transcript:\\n\\nInstructions: Learners, you may begin your patient station, remove your cover sheet, jot down any... Thank you. Come in.  Come on in, man.  Hello, Mr. Pimbleton. Hi, my name is Coleman. I\\'m a medical student at UT Southwestern. I am here to ask you some questions. I just want to know how you are feeling today.\\n\\nStudent: I\\'ve been feeling sick to my stomach.\\n\\nPatient: I am feeling sick.\\n\\nStudent: Oh no, how long have you felt sick?\\n\\nPatient: About two days.\\n\\nStudent: Okay, when did this start?\\n\\nPatient: It started getting nauseous.  I was feeling nausea.  I had a stomachache.  I felt nause up. I am not good.\\n\\nStudent: Okay, when did this start? I started getting nauseous two days ago. Threw up the first  time yesterday after breakfast, about 20 minutes after I ate. Any other episodes of vomiting?\\n\\nPatient: Yeah, I'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarize(summary, read_transcript_from_id(transcript_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_nosum(transcript, max_new_tokens=10000):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": '''The following text represents an audio transcript between a medical student and a patient.\n",
    "                The student is the one who will be playing the role of doctor, asking questions about the patient's condition and symptoms.\n",
    "                Please diarize the following transcript in order to indicate who speaks when, using the format in the following example, with the label \"Student\" for the medical student and \"Patient\" for the patient (not their names):\n",
    "\n",
    "                Student: Hello, how are you today?\n",
    "\n",
    "                Patient: I am feeling sick. \n",
    "\n",
    "                Student: Oh no, how long have you felt sick? \n",
    "\n",
    "                Patient: About two days. \n",
    "\n",
    "                Separate speaker turns by an extra new line, as above ('\\n\\n'). The lines do not always have to be alternating labels. There may be consecutive lines from one speaker. Do not use any speaker labels for the two participants other than \"Student\" and \"Patient.\"\n",
    "\n",
    "                The transcript may contain some instructions for the student, coming from a third-party speaker. Please label this segment \"Instructions: \". \n",
    "                If there are no instructions in the transcript, disregard this label. \n",
    "\n",
    "                IMPORTANT: Do not remove or add any words to the transcript other than the speaker labels. Also, do not add or remove any punctuation or change any spellings.'''},\n",
    "    {\"role\": \"user\", \"content\": 'transcript: ' + transcript},\n",
    "]      \n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(diarization, transcript_id, path = '/archive/shared/sim_center/shared/annie/experiments/llama-8B-instruct/'):\n",
    "    #path = '/archive/shared/sim_center/shared/annie/claude-opus/'\n",
    "    with open(path + transcript_id + \".txt\", \"w\") as outfile:\n",
    "        outfile.write('ID: ' + transcript_id + '\\n\\n' + diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_diarize(transcript_id):\n",
    "    print(transcript_id)\n",
    "    transcript = read_transcript_from_id(transcript_id)\n",
    "    summary = summarize(transcript)\n",
    "    diarization = diarize(summary, transcript)[\"content\"]\n",
    "    save_file(diarization, transcript_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set1 = ['01_0542_298135',\n",
    "'02_0036_174595',\n",
    "'03_0028_174553',\n",
    "'04_0043_174686',\n",
    "'05_0033_174804',\n",
    "'06_0079_175106',\n",
    "'07_0068_174641',\n",
    "'08_0029_174576',\n",
    "'09_0029_174582',\n",
    "'10_0991_331330']\n",
    "\n",
    "id_set2 = ['01_1080_366142',\n",
    "           '02_1056_380177',\n",
    "           '03_1500_380168',\n",
    "           '04_1512_380182',\n",
    "           '05_1066_380195',\n",
    "           '06_1048_365209',\n",
    "           '07_1111_380134',\n",
    "           '08_1044_380133',\n",
    "           '09_1039_380193',\n",
    "           '10_1005_331402'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp for formatting\n",
    "def remove_extraspace(path, ids):\n",
    "    for id in ids: \n",
    "        this_path = path + id + '.txt'\n",
    "        with open(this_path, 'r') as file:\n",
    "            transcript = file.read()\n",
    "        sections = transcript.split('  ')\n",
    "        transcript = ''\n",
    "        for section in sections:\n",
    "            transcript += ' ' + section\n",
    "        with open(path + id + \".txt\", \"w\") as outfile:\n",
    "            outfile.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_1080_366142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_1056_380177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_1500_380168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_1512_380182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m id_set2:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mllama_diarize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mllama_diarize\u001b[0;34m(transcript_id)\u001b[0m\n\u001b[1;32m      3\u001b[0m transcript \u001b[38;5;241m=\u001b[39m read_transcript_from_id(transcript_id)\n\u001b[1;32m      4\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize(transcript)\n\u001b[0;32m----> 5\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[43mdiarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m save_file(diarization, transcript_id)\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mdiarize\u001b[0;34m(summary, transcript)\u001b[0m\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mThe following text represents an audio transcript between a medical student and a patient, with the summary included below.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m                The student is the one who will be playing the role of doctor, asking questions about the patient\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms condition and symptoms.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtranscript: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m transcript},\n\u001b[1;32m     22\u001b[0m ]      \n\u001b[1;32m     23\u001b[0m     terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m         pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     25\u001b[0m         pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:236\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 236\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pf/lib/python3.9/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for id in id_set2:\n",
    "    llama_diarize(id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
